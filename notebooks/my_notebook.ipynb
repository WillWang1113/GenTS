{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import einops\n",
    "from src.layers.flow import get_mask, MaskedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32,10,2)\n",
    "x_hat = torch.randn(32,10,2)\n",
    "\n",
    "mse_loss = torch.nn.functional.mse_loss(x_hat, x) * 10 * 2\n",
    "sum_loss = torch.nn.functional.mse_loss(x_hat, x, reduction='sum') / 32\n",
    "print(mse_loss)\n",
    "print(sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum()/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(dim=tuple(range(1, x.ndim))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "res = np.load('/home/user/data2/ICML_rebuttal/savings/uncond/etth2_24_S/KoVAEorig/cond_None_dtm_True_syn.npy')\n",
    "# res = np.load('/home/user/data/ICML_rebuttal/savings/uncond/etth2_24_S/KoVAE_orig/cond_None_dtm_True_syn.npy')\n",
    "start = np.random.randint(0, len(res))\n",
    "_ = plt.plot(res[start, :, :].squeeze().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/user/workspaces/THU-timeseries/ETT-small/ETTh2.csv', index_col=0)\n",
    "# df.values.shape\n",
    "start = np.random.randint(0, len(df)-24)\n",
    "_ = plt.plot(df.values[start:start+24, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Literal\n",
    "from lightning import LightningModule\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def _condition_shape_check(n_sample, condition, cond_type):\n",
    "    assert n_sample >=1\n",
    "    if cond_type is None:\n",
    "    \n",
    "        if condition.shape[0] == 1:\n",
    "            condition = condition.repeat(\n",
    "                n_sample, *[1 for _ in range(len(condition.shape) - 1)]\n",
    "            )\n",
    "        elif condition.shape[0] == n_sample:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The batch size of the given condition should be the same as n_sample or just 1.\"\n",
    "            )\n",
    " \n",
    "    return condition\n",
    "\n",
    "\n",
    "class BaseModel(ABC, LightningModule):\n",
    "    \"\"\"Base class for generative models in PyTorch Lightning\"\"\"\n",
    "    ALLOW_CONDITION = ...\n",
    "    \n",
    "    def __init__(self, seq_len, seq_dim, condition, lr, **kwargs):\n",
    "        super().__init__()\n",
    "        if condition not in self.ALLOW_CONDITION:\n",
    "            raise ValueError(f\"Condition '{condition}' not allowed. Choose from {self.ALLOW_CONDITION}\")\n",
    "    \n",
    "    @torch.no_grad()  # wrap with torch.no_grad()\n",
    "    def sample(self, n_sample: int = 1, condition=None, **kwargs):\n",
    "        \"\"\"Generate samples from the generative model\"\"\"\n",
    "        condition = _condition_shape_check(n_sample, condition, self.condition)\n",
    "        self.eval()\n",
    "        return self._sample_impl(n_sample, condition, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _sample_impl(self, n_sample: int = 1, condition=None, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"Actual implementation of the sampling process\"\"\"\n",
    "\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    \"\"\"AAAAAAA\n",
    "\n",
    "    Args:\n",
    "        BaseModel (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    ALLOW_CONDITION = [None, 'predict']\n",
    "    def __init__(self, seq_len, seq_dim, condition, lr, **kwargs):\n",
    "        \"\"\"aaaaaaa\n",
    "\n",
    "        Args:\n",
    "            seq_len (_type_): _description_\n",
    "            seq_dim (_type_): _description_\n",
    "            condition (_type_): _description_\n",
    "            lr (_type_): _description_\n",
    "        \"\"\"\n",
    "        super().__init__(seq_len, seq_dim, condition, lr, **kwargs)\n",
    "    \n",
    "    def _sample_impl(self, n_sample = 1, condition=None, **kwargs):\n",
    "        return super()._sample_impl(n_sample, condition, **kwargs)\n",
    "\n",
    "MyModel\n",
    "model = MyModel(24, 1, 'impute', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "total_seq_len = 64\n",
    "num_samples = 200\n",
    "t = torch.linspace(0, 4 * torch.pi, total_seq_len).float()\n",
    "curves = []\n",
    "labels = []\n",
    "for _ in range(num_samples):\n",
    "    a = torch.rand(1).item() * 0.5  # Initial radius\n",
    "    b = torch.rand(1).item() * 0.2  # Growth rate\n",
    "\n",
    "    direction = torch.randint(0, 2, (1,)).item()  # 0=clockwise, 1=ccw\n",
    "\n",
    "    r = a + b * t\n",
    "    if direction == 0:\n",
    "        x = r * torch.cos(t)\n",
    "        y = r * torch.sin(t)\n",
    "    else:\n",
    "        x = -r * torch.cos(t)\n",
    "        y = r * torch.sin(t)\n",
    "\n",
    "    x += torch.randn_like(x) * 0.01\n",
    "    y += torch.randn_like(y) * 0.01\n",
    "\n",
    "    curve = torch.stack([x, y], dim=1)\n",
    "    curves.append(curve)\n",
    "    labels.append(direction)\n",
    "data, class_cond = torch.stack(curves), torch.tensor(labels).unsqueeze(-1)\n",
    "print(data.shape, class_cond.shape)\n",
    "\n",
    "class_cond[1,...,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "\n",
    "n_fft = 200\n",
    "hop_length = 10\n",
    "data = torch.randn(32, 64, 5)\n",
    "hop_length = 8\n",
    "print((data.shape[1]) // hop_length + 1)\n",
    "data = torch.permute(\n",
    "    data, (0, 2, 1)\n",
    ")  # we permute to match requirements of torchaudio.transforms.Spectrogram\n",
    "spec = T.Spectrogram(n_fft=15, hop_length=hop_length, center=True, power=None).to(data.device)\n",
    "transformed_data = spec(data)\n",
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sine_data_generation(no, seq_len, dim, freq_scale=1):\n",
    "\n",
    "    \"\"\"Sine data generation.\n",
    "  \n",
    "    Args:\n",
    "    \n",
    "    - no: the number of samples\n",
    "    - seq_len: sequence length of the time-series\n",
    "    - dim: feature dimensions\n",
    "    \n",
    "    Returns:\n",
    "    - data: generated data\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize the output\n",
    "    data = list()\n",
    "\n",
    "    # Generate sine data\n",
    "    for i in range(no):      \n",
    "        # Initialize each time-series\n",
    "        temp = list()\n",
    "        # For each feature\n",
    "        for k in range(dim):\n",
    "            # Randomly drawn frequency and phase\n",
    "            freq = np.random.uniform(0.05, 0.4)            \n",
    "            phase = np.random.uniform(0, 1.5)\n",
    "                \n",
    "            # Generate sine signal based on the drawn frequency and phase\n",
    "            temp_data = [np.sin(freq * j + phase) for j in range(seq_len)] \n",
    "            temp.append(temp_data)\n",
    "            \n",
    "        # Align row/column\n",
    "        temp = np.transpose(np.asarray(temp))        \n",
    "        # Normalize to [0,1]\n",
    "        temp = (temp + 1)*0.5\n",
    "        # Stack the generated data\n",
    "        data.append(temp)\n",
    "                    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.flow.fourierflow._fourier import DFT, reconstruct_DFT\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "t = np.linspace(0, 6, 65)\n",
    "k = int(len(t) / 2) + 1\n",
    "\n",
    "x = np.sin(t) + np.random.randn(*t.shape) * 0.1\n",
    "\n",
    "x = x.reshape(-1, len(t))\n",
    "x = torch.from_numpy(x)\n",
    "m = DFT(x.shape[1])\n",
    "z, log_pz, log_jacob = m(x)\n",
    "z = torch.complex(z[:, 0, :], z[:, 1, :])\n",
    "print(\"FF implement\")\n",
    "print(z.shape)\n",
    "print(z)\n",
    "\n",
    "print(\"torch.fft.rfft implement\")\n",
    "x_torch_fft = torch.fft.rfft(x) / len(t)\n",
    "print(x_torch_fft.shape)\n",
    "x_torch_fft = x_torch_fft.flip(dims=[1]).conj().type_as(z)\n",
    "print(x_torch_fft.shape)\n",
    "print(x_torch_fft)\n",
    "torch.testing.assert_close(x_torch_fft, z)\n",
    "\n",
    "# x_torch_fft = np.fft.fft(x)\n",
    "# x_torch_fft = np.fft.fftshift(x_torch_fft)\n",
    "# print(x_torch_fft.shape)\n",
    "# print(x_torch_fft)\n",
    "\n",
    "# x_torch = torch.complex(x_numpy[:,0,:], x_numpy[:,1,:])\n",
    "# print(x_torch.shape)\n",
    "# x_irfft = torch.fft.irfft(x_torch, dim=1)\n",
    "# x_irfft.shape\n",
    "\n",
    "# x_numpy_r = reconstruct_DFT(x_numpy[0, :, :], component=\"real\").detach().numpy()\n",
    "\n",
    "# x_numpy_i = reconstruct_DFT(x_numpy[0, :, :], component=\"imag\").detach().numpy()\n",
    "# print(x_numpy_r.shape)\n",
    "# print(x_numpy_i.shape)\n",
    "# np.real(np.fft.ifft(np.fft.ifftshift(x_numpy_r + 1j * x_numpy_i))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.model.flow.temporalflow._backbones import TempFlowTrainingNetwork\n",
    "\n",
    "x = torch.randn(1, 7, 3)\n",
    "obs_x = torch.randn(1, 14, 3)\n",
    "\n",
    "m = TempFlowTrainingNetwork(\n",
    "    input_size=x.shape[-1],\n",
    "    num_layers=1,\n",
    "    num_cells=3,\n",
    "    cell_type=\"GRU\",\n",
    "    context_length=obs_x.shape[1],\n",
    "    history_length=14,\n",
    "    prediction_length=x.shape[1],\n",
    "    dropout_rate=0.1,\n",
    "    lags_seq=[1],\n",
    "    target_dim=x.shape[-1],\n",
    "    conditioning_length=200,\n",
    "    flow_type='MAF',\n",
    "    n_blocks=1,\n",
    "    hidden_size=64,\n",
    "    n_hidden=64,\n",
    "    dequantize=False\n",
    ")\n",
    "\n",
    "distr_args = m.distr_args(x)\n",
    "m.flow.log_prob(x, distr_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 2])\n",
      "torch.Size([32, 10, 2])\n",
      "torch.Size([32, 9, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchcde import linear_interpolation_coeffs, natural_cubic_coeffs\n",
    "\n",
    "x = torch.randn(32, 10, 2)\n",
    "x[11,3,0] = float('nan')\n",
    "x[1,6,1] = float('nan')\n",
    "x[3,1,0] = float('nan')\n",
    "\n",
    "print(x.shape)\n",
    "print(linear_interpolation_coeffs(x).shape)\n",
    "print(natural_cubic_coeffs(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 25)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.randn(100, 24, 7)\n",
    "X = [np.hstack((0, data[k][:, 0])) for k in range(len(data))]\n",
    "X = np.array(X)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.566667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.5300</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.992500</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>45.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.723333</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>46.560000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>42.025714</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>41.163333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>43.096812</td>\n",
       "      <td>43.096812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>25.754000</td>\n",
       "      <td>42.080000</td>\n",
       "      <td>27.133333</td>\n",
       "      <td>41.223333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>49.282940</td>\n",
       "      <td>49.282940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.596667</td>\n",
       "      <td>25.628571</td>\n",
       "      <td>42.768571</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>41.690000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.466667</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>29.199117</td>\n",
       "      <td>29.199117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>420</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.990000</td>\n",
       "      <td>25.414000</td>\n",
       "      <td>43.036000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>41.290000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8175</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>26.166667</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>6.322784</td>\n",
       "      <td>6.322784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>430</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>25.264286</td>\n",
       "      <td>42.971429</td>\n",
       "      <td>26.823333</td>\n",
       "      <td>41.156667</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.963333</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8450</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>34.118851</td>\n",
       "      <td>34.118851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Appliances  lights         T1       RH_1         T2       RH_2  \\\n",
       "0              60      30  19.890000  47.596667  19.200000  44.790000   \n",
       "1              60      30  19.890000  46.693333  19.200000  44.722500   \n",
       "2              50      30  19.890000  46.300000  19.200000  44.626667   \n",
       "3              50      40  19.890000  46.066667  19.200000  44.590000   \n",
       "4              60      40  19.890000  46.333333  19.200000  44.530000   \n",
       "...           ...     ...        ...        ...        ...        ...   \n",
       "19730         100       0  25.566667  46.560000  25.890000  42.025714   \n",
       "19731          90       0  25.500000  46.500000  25.754000  42.080000   \n",
       "19732         270      10  25.500000  46.596667  25.628571  42.768571   \n",
       "19733         420      10  25.500000  46.990000  25.414000  43.036000   \n",
       "19734         430      10  25.500000  46.600000  25.264286  42.971429   \n",
       "\n",
       "              T3       RH_3         T4       RH_4  ...         T9     RH_9  \\\n",
       "0      19.790000  44.730000  19.000000  45.566667  ...  17.033333  45.5300   \n",
       "1      19.790000  44.790000  19.000000  45.992500  ...  17.066667  45.5600   \n",
       "2      19.790000  44.933333  18.926667  45.890000  ...  17.000000  45.5000   \n",
       "3      19.790000  45.000000  18.890000  45.723333  ...  17.000000  45.4000   \n",
       "4      19.790000  45.000000  18.890000  45.530000  ...  17.000000  45.4000   \n",
       "...          ...        ...        ...        ...  ...        ...      ...   \n",
       "19730  27.200000  41.163333  24.700000  45.590000  ...  23.200000  46.7900   \n",
       "19731  27.133333  41.223333  24.700000  45.590000  ...  23.200000  46.7900   \n",
       "19732  27.050000  41.690000  24.700000  45.730000  ...  23.200000  46.7900   \n",
       "19733  26.890000  41.290000  24.700000  45.790000  ...  23.200000  46.8175   \n",
       "19734  26.823333  41.156667  24.700000  45.963333  ...  23.200000  46.8450   \n",
       "\n",
       "           T_out  Press_mm_hg     RH_out  Windspeed  Visibility  Tdewpoint  \\\n",
       "0       6.600000        733.5  92.000000   7.000000   63.000000   5.300000   \n",
       "1       6.483333        733.6  92.000000   6.666667   59.166667   5.200000   \n",
       "2       6.366667        733.7  92.000000   6.333333   55.333333   5.100000   \n",
       "3       6.250000        733.8  92.000000   6.000000   51.500000   5.000000   \n",
       "4       6.133333        733.9  92.000000   5.666667   47.666667   4.900000   \n",
       "...          ...          ...        ...        ...         ...        ...   \n",
       "19730  22.733333        755.2  55.666667   3.333333   23.666667  13.333333   \n",
       "19731  22.600000        755.2  56.000000   3.500000   24.500000  13.300000   \n",
       "19732  22.466667        755.2  56.333333   3.666667   25.333333  13.266667   \n",
       "19733  22.333333        755.2  56.666667   3.833333   26.166667  13.233333   \n",
       "19734  22.200000        755.2  57.000000   4.000000   27.000000  13.200000   \n",
       "\n",
       "             rv1        rv2  \n",
       "0      13.275433  13.275433  \n",
       "1      18.606195  18.606195  \n",
       "2      28.642668  28.642668  \n",
       "3      45.410389  45.410389  \n",
       "4      10.084097  10.084097  \n",
       "...          ...        ...  \n",
       "19730  43.096812  43.096812  \n",
       "19731  49.282940  49.282940  \n",
       "19732  29.199117  29.199117  \n",
       "19733   6.322784   6.322784  \n",
       "19734  34.118851  34.118851  \n",
       "\n",
       "[19735 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# url = 'https://raw.githubusercontent.com/jsyoon0823/TimeGAN/refs/heads/master/data/stock_data.csv'\n",
    "url = 'https://raw.githubusercontent.com/jsyoon0823/TimeGAN/refs/heads/master/data/energy_data.csv'\n",
    "headers = {\"Authorization\": \"Test\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "df = pd.read_csv(StringIO(response.text))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "        518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
      "        532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "        546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
      "        560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
      "        574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
      "        588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
      "        602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615,\n",
      "        616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "        630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643,\n",
      "        644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657,\n",
      "        658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671,\n",
      "        672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
      "        686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699,\n",
      "        700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713,\n",
      "        714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
      "        728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n",
      "        742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755,\n",
      "        756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769,\n",
      "        770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783,\n",
      "        784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797,\n",
      "        798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "        812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825,\n",
      "        826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839,\n",
      "        840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853,\n",
      "        854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867,\n",
      "        868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881,\n",
      "        882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895,\n",
      "        896, 897, 898, 899, 900, 901, 902, 903])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ...,  42,  43,  44],\n",
       "        [  1,   2,   3,  ...,  43,  44,  45],\n",
       "        [  2,   3,   4,  ...,  44,  45,  46],\n",
       "        ...,\n",
       "        [857, 858, 859,  ..., 899, 900, 901],\n",
       "        [858, 859, 860,  ..., 900, 901, 902],\n",
       "        [859, 860, 861,  ..., 901, 902, 903]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "seq_len = 45\n",
    "a = torch.arange(1000)\n",
    "n_window = 1000 - seq_len + 1\n",
    "n_trainval_window = int(n_window * 0.7) + int(n_window * 0.2)\n",
    "n_trainval_timesteps = n_trainval_window + seq_len - 1\n",
    "\n",
    "print(a[:n_trainval_timesteps])\n",
    "\n",
    "all_x = []\n",
    "for i in range(len(a)-seq_len+1):\n",
    "    all_x.append(a[i:i+seq_len])\n",
    "all_x = torch.stack(all_x, dim=0)\n",
    "all_x[:n_trainval_window]\n",
    "# print(all_x[-2:])  # Should be (955, 45, 5) if seq_len is 45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
