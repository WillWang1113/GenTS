{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import einops\n",
    "from src.layers.flow import get_mask, MaskedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32,10,2)\n",
    "x_hat = torch.randn(32,10,2)\n",
    "\n",
    "mse_loss = torch.nn.functional.mse_loss(x_hat, x) * 10 * 2\n",
    "sum_loss = torch.nn.functional.mse_loss(x_hat, x, reduction='sum') / 32\n",
    "print(mse_loss)\n",
    "print(sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum()/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(dim=tuple(range(1, x.ndim))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "res = np.load('/home/user/data2/ICML_rebuttal/savings/uncond/etth2_24_S/KoVAEorig/cond_None_dtm_True_syn.npy')\n",
    "# res = np.load('/home/user/data/ICML_rebuttal/savings/uncond/etth2_24_S/KoVAE_orig/cond_None_dtm_True_syn.npy')\n",
    "start = np.random.randint(0, len(res))\n",
    "_ = plt.plot(res[start, :, :].squeeze().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/user/workspaces/THU-timeseries/ETT-small/ETTh2.csv', index_col=0)\n",
    "# df.values.shape\n",
    "start = np.random.randint(0, len(df)-24)\n",
    "_ = plt.plot(df.values[start:start+24, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Literal\n",
    "from lightning import LightningModule\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def _condition_shape_check(n_sample, condition, cond_type):\n",
    "    assert n_sample >=1\n",
    "    if cond_type is None:\n",
    "    \n",
    "        if condition.shape[0] == 1:\n",
    "            condition = condition.repeat(\n",
    "                n_sample, *[1 for _ in range(len(condition.shape) - 1)]\n",
    "            )\n",
    "        elif condition.shape[0] == n_sample:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The batch size of the given condition should be the same as n_sample or just 1.\"\n",
    "            )\n",
    " \n",
    "    return condition\n",
    "\n",
    "\n",
    "class BaseModel(ABC, LightningModule):\n",
    "    \"\"\"Base class for generative models in PyTorch Lightning\"\"\"\n",
    "    ALLOW_CONDITION = ...\n",
    "    \n",
    "    def __init__(self, seq_len, seq_dim, condition, lr, **kwargs):\n",
    "        super().__init__()\n",
    "        if condition not in self.ALLOW_CONDITION:\n",
    "            raise ValueError(f\"Condition '{condition}' not allowed. Choose from {self.ALLOW_CONDITION}\")\n",
    "    \n",
    "    @torch.no_grad()  # wrap with torch.no_grad()\n",
    "    def sample(self, n_sample: int = 1, condition=None, **kwargs):\n",
    "        \"\"\"Generate samples from the generative model\"\"\"\n",
    "        condition = _condition_shape_check(n_sample, condition, self.condition)\n",
    "        self.eval()\n",
    "        return self._sample_impl(n_sample, condition, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _sample_impl(self, n_sample: int = 1, condition=None, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"Actual implementation of the sampling process\"\"\"\n",
    "\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    \"\"\"AAAAAAA\n",
    "\n",
    "    Args:\n",
    "        BaseModel (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    ALLOW_CONDITION = [None, 'predict']\n",
    "    def __init__(self, seq_len, seq_dim, condition, lr, **kwargs):\n",
    "        \"\"\"aaaaaaa\n",
    "\n",
    "        Args:\n",
    "            seq_len (_type_): _description_\n",
    "            seq_dim (_type_): _description_\n",
    "            condition (_type_): _description_\n",
    "            lr (_type_): _description_\n",
    "        \"\"\"\n",
    "        super().__init__(seq_len, seq_dim, condition, lr, **kwargs)\n",
    "    \n",
    "    def _sample_impl(self, n_sample = 1, condition=None, **kwargs):\n",
    "        return super()._sample_impl(n_sample, condition, **kwargs)\n",
    "\n",
    "MyModel\n",
    "model = MyModel(24, 1, 'impute', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "total_seq_len = 64\n",
    "num_samples = 200\n",
    "t = torch.linspace(0, 4 * torch.pi, total_seq_len).float()\n",
    "curves = []\n",
    "labels = []\n",
    "for _ in range(num_samples):\n",
    "    a = torch.rand(1).item() * 0.5  # Initial radius\n",
    "    b = torch.rand(1).item() * 0.2  # Growth rate\n",
    "\n",
    "    direction = torch.randint(0, 2, (1,)).item()  # 0=clockwise, 1=ccw\n",
    "\n",
    "    r = a + b * t\n",
    "    if direction == 0:\n",
    "        x = r * torch.cos(t)\n",
    "        y = r * torch.sin(t)\n",
    "    else:\n",
    "        x = -r * torch.cos(t)\n",
    "        y = r * torch.sin(t)\n",
    "\n",
    "    x += torch.randn_like(x) * 0.01\n",
    "    y += torch.randn_like(y) * 0.01\n",
    "\n",
    "    curve = torch.stack([x, y], dim=1)\n",
    "    curves.append(curve)\n",
    "    labels.append(direction)\n",
    "data, class_cond = torch.stack(curves), torch.tensor(labels).unsqueeze(-1)\n",
    "print(data.shape, class_cond.shape)\n",
    "\n",
    "class_cond[1,...,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "\n",
    "n_fft = 200\n",
    "hop_length = 10\n",
    "data = torch.randn(32, 64, 5)\n",
    "hop_length = 8\n",
    "print((data.shape[1]) // hop_length + 1)\n",
    "data = torch.permute(\n",
    "    data, (0, 2, 1)\n",
    ")  # we permute to match requirements of torchaudio.transforms.Spectrogram\n",
    "spec = T.Spectrogram(n_fft=15, hop_length=hop_length, center=True, power=None).to(data.device)\n",
    "transformed_data = spec(data)\n",
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sine_data_generation(no, seq_len, dim, freq_scale=1):\n",
    "\n",
    "    \"\"\"Sine data generation.\n",
    "  \n",
    "    Args:\n",
    "    \n",
    "    - no: the number of samples\n",
    "    - seq_len: sequence length of the time-series\n",
    "    - dim: feature dimensions\n",
    "    \n",
    "    Returns:\n",
    "    - data: generated data\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize the output\n",
    "    data = list()\n",
    "\n",
    "    # Generate sine data\n",
    "    for i in range(no):      \n",
    "        # Initialize each time-series\n",
    "        temp = list()\n",
    "        # For each feature\n",
    "        for k in range(dim):\n",
    "            # Randomly drawn frequency and phase\n",
    "            freq = np.random.uniform(0.05, 0.4)            \n",
    "            phase = np.random.uniform(0, 1.5)\n",
    "                \n",
    "            # Generate sine signal based on the drawn frequency and phase\n",
    "            temp_data = [np.sin(freq * j + phase) for j in range(seq_len)] \n",
    "            temp.append(temp_data)\n",
    "            \n",
    "        # Align row/column\n",
    "        temp = np.transpose(np.asarray(temp))        \n",
    "        # Normalize to [0,1]\n",
    "        temp = (temp + 1)*0.5\n",
    "        # Stack the generated data\n",
    "        data.append(temp)\n",
    "                    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.flow.fourierflow._fourier import DFT, reconstruct_DFT\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "t = np.linspace(0, 6, 65)\n",
    "k = int(len(t) / 2) + 1\n",
    "\n",
    "x = np.sin(t) + np.random.randn(*t.shape) * 0.1\n",
    "\n",
    "x = x.reshape(-1, len(t))\n",
    "x = torch.from_numpy(x)\n",
    "m = DFT(x.shape[1])\n",
    "z, log_pz, log_jacob = m(x)\n",
    "z = torch.complex(z[:, 0, :], z[:, 1, :])\n",
    "print(\"FF implement\")\n",
    "print(z.shape)\n",
    "print(z)\n",
    "\n",
    "print(\"torch.fft.rfft implement\")\n",
    "x_torch_fft = torch.fft.rfft(x) / len(t)\n",
    "print(x_torch_fft.shape)\n",
    "x_torch_fft = x_torch_fft.flip(dims=[1]).conj().type_as(z)\n",
    "print(x_torch_fft.shape)\n",
    "print(x_torch_fft)\n",
    "torch.testing.assert_close(x_torch_fft, z)\n",
    "\n",
    "# x_torch_fft = np.fft.fft(x)\n",
    "# x_torch_fft = np.fft.fftshift(x_torch_fft)\n",
    "# print(x_torch_fft.shape)\n",
    "# print(x_torch_fft)\n",
    "\n",
    "# x_torch = torch.complex(x_numpy[:,0,:], x_numpy[:,1,:])\n",
    "# print(x_torch.shape)\n",
    "# x_irfft = torch.fft.irfft(x_torch, dim=1)\n",
    "# x_irfft.shape\n",
    "\n",
    "# x_numpy_r = reconstruct_DFT(x_numpy[0, :, :], component=\"real\").detach().numpy()\n",
    "\n",
    "# x_numpy_i = reconstruct_DFT(x_numpy[0, :, :], component=\"imag\").detach().numpy()\n",
    "# print(x_numpy_r.shape)\n",
    "# print(x_numpy_i.shape)\n",
    "# np.real(np.fft.ifft(np.fft.ifftshift(x_numpy_r + 1j * x_numpy_i))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.model.flow.temporalflow._backbones import TempFlowTrainingNetwork\n",
    "\n",
    "x = torch.randn(1, 7, 3)\n",
    "obs_x = torch.randn(1, 14, 3)\n",
    "\n",
    "m = TempFlowTrainingNetwork(\n",
    "    input_size=x.shape[-1],\n",
    "    num_layers=1,\n",
    "    num_cells=3,\n",
    "    cell_type=\"GRU\",\n",
    "    context_length=obs_x.shape[1],\n",
    "    history_length=14,\n",
    "    prediction_length=x.shape[1],\n",
    "    dropout_rate=0.1,\n",
    "    lags_seq=[1],\n",
    "    target_dim=x.shape[-1],\n",
    "    conditioning_length=200,\n",
    "    flow_type='MAF',\n",
    "    n_blocks=1,\n",
    "    hidden_size=64,\n",
    "    n_hidden=64,\n",
    "    dequantize=False\n",
    ")\n",
    "\n",
    "distr_args = m.distr_args(x)\n",
    "m.flow.log_prob(x, distr_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 2])\n",
      "torch.Size([32, 10, 2])\n",
      "torch.Size([32, 9, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchcde import linear_interpolation_coeffs, natural_cubic_coeffs\n",
    "\n",
    "x = torch.randn(32, 10, 2)\n",
    "x[11,3,0] = float('nan')\n",
    "x[1,6,1] = float('nan')\n",
    "x[3,1,0] = float('nan')\n",
    "\n",
    "print(x.shape)\n",
    "print(linear_interpolation_coeffs(x).shape)\n",
    "print(natural_cubic_coeffs(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7389, -1.2725,  1.5963],\n",
      "        [-0.6765,  0.4567, -1.3726],\n",
      "        [ 0.4536,  0.9050, -0.3567],\n",
      "        [-0.0080, -1.8440,  0.3939],\n",
      "        [ 0.4295, -0.7204,  1.6986],\n",
      "        [-0.2835,  1.2082, -0.4737],\n",
      "        [ 1.3043, -0.8243, -0.4384],\n",
      "        [ 0.8145, -1.2454,  0.4439],\n",
      "        [ 1.0288,  0.5532,  0.1855],\n",
      "        [-1.0235, -1.4936, -0.3827],\n",
      "        [-0.2183,  0.6908, -0.1825],\n",
      "        [-0.5283,  0.9068,  1.3441],\n",
      "        [ 0.2685, -0.1085,  0.4688],\n",
      "        [ 0.0340,  1.1647, -1.2221],\n",
      "        [-0.4629,  0.9666, -0.5805],\n",
      "        [-1.0519,  0.4907, -0.4023],\n",
      "        [-1.2784,  0.7958, -0.0916],\n",
      "        [ 0.0074, -1.1235,  1.0418],\n",
      "        [-0.1136,  0.4084,  0.3919],\n",
      "        [ 1.8397, -0.3791, -0.3604],\n",
      "        [ 1.5482,  0.6093,  1.9251],\n",
      "        [-1.3903,  0.8975, -0.6707],\n",
      "        [-0.8545,  0.0107, -0.2289],\n",
      "        [ 0.7183, -0.0794,  0.2755]])\n",
      "tensor([[-2.7389, -1.2725,  1.5963],\n",
      "        [-0.6765,  0.4567, -1.3726],\n",
      "        [ 0.4536,  0.9050, -0.3567],\n",
      "        [    nan, -1.8440,  0.3939],\n",
      "        [ 0.4295, -0.7204,  1.6986],\n",
      "        [-0.2835,  1.2082, -0.4737],\n",
      "        [    nan, -0.8243, -0.4384],\n",
      "        [    nan, -1.2454,  0.4439],\n",
      "        [ 1.0288,  0.5532,  0.1855],\n",
      "        [-1.0235,     nan, -0.3827],\n",
      "        [-0.2183,  0.6908, -0.1825],\n",
      "        [    nan,  0.9068,  1.3441],\n",
      "        [ 0.2685, -0.1085,  0.4688],\n",
      "        [ 0.0340,  1.1647, -1.2221],\n",
      "        [-0.4629,  0.9666, -0.5805],\n",
      "        [-1.0519,  0.4907, -0.4023],\n",
      "        [-1.2784,  0.7958, -0.0916],\n",
      "        [ 0.0074, -1.1235,  1.0418],\n",
      "        [-0.1136,  0.4084,     nan],\n",
      "        [ 1.8397, -0.3791, -0.3604],\n",
      "        [ 1.5482,     nan,  1.9251],\n",
      "        [-1.3903,  0.8975, -0.6707],\n",
      "        [-0.8545,  0.0107, -0.2289],\n",
      "        [ 0.7183, -0.0794,  0.2755]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [-0.0080,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [ 1.3043,     nan,     nan],\n",
       "        [ 0.8145,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan, -1.4936,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [-0.5283,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,  0.3919],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,  0.6093,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "orig_data = torch.randn(500, 24, 3)\n",
    "mask = torch.rand_like(orig_data) < 0.1\n",
    "nan_orig_data = orig_data.masked_fill(mask.bool(), float('nan'))\n",
    "print(orig_data[0])\n",
    "print(nan_orig_data[0])\n",
    "mask_infer = torch.isnan(nan_orig_data)\n",
    "\n",
    "\n",
    "orig_data.masked_fill(~mask_infer.bool(), float(\"nan\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# mqloss, mae, mse are from neuralforecast.losses.numpy\n",
    "def mqloss(\n",
    "    y: np.ndarray,\n",
    "    y_hat: np.ndarray,\n",
    "    quantiles: np.ndarray,\n",
    "    weights: Optional[np.ndarray] = None,\n",
    "    axis: Optional[int] = None,\n",
    ") -> Union[float, np.ndarray]:\n",
    "    \"\"\"Multi-Quantile loss\n",
    "\n",
    "    Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
    "    MQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute\n",
    "    difference between predicted quantiles and observed values.\n",
    "\n",
    "    $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "\n",
    "    The limit behavior of MQL allows to measure the accuracy\n",
    "    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with\n",
    "    the continuous ranked probability score (CRPS). This can be achieved\n",
    "    through a numerical integration technique, that discretizes the quantiles\n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over\n",
    "    uniformly distanced quantiles.\n",
    "\n",
    "    $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values.<br>\n",
    "    `y_hat`: numpy array, Predicted values.<br>\n",
    "    `quantiles`: numpy array,(n_quantiles). Quantiles to estimate from the distribution of y.<br>\n",
    "    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `mqloss`: numpy array, (single value).\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "    # _metric_protections(y, y_hat, weights)\n",
    "    n_q = len(quantiles)\n",
    "\n",
    "    y_rep = np.expand_dims(y, axis=-1)\n",
    "    error = y_hat - y_rep\n",
    "    sq = np.maximum(-error, np.zeros_like(error))\n",
    "    s1_q = np.maximum(error, np.zeros_like(error))\n",
    "    mqloss = quantiles * sq + (1 - quantiles) * s1_q\n",
    "\n",
    "    # Match y/weights dimensions and compute weighted average\n",
    "    weights = np.repeat(np.expand_dims(weights, axis=-1), repeats=n_q, axis=-1)\n",
    "    mqloss = np.average(mqloss, weights=weights, axis=axis)\n",
    "\n",
    "    return mqloss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.251982\n",
      "0.2519820121776677\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_pinball_loss\n",
    "\n",
    "\n",
    "x = torch.randn(32, 30, 3, 100)\n",
    "y = torch.randn(32, 30, 3)\n",
    "quantiles = np.array([0.1, 0.5, 0.9])\n",
    "\n",
    "y_pred = x\n",
    "y_pred_quantiles = np.quantile(y_pred, quantiles, axis=-1)\n",
    "y_pred_quantiles = torch.from_numpy(y_pred_quantiles).float()\n",
    "all_qloss = []\n",
    "for i, q in enumerate(quantiles):\n",
    "    qloss = mean_pinball_loss(y.flatten(1), y_pred_quantiles[i].flatten(1), alpha=q)\n",
    "    all_qloss.append(qloss)\n",
    "all_qloss = np.mean(all_qloss)\n",
    "print(all_qloss)\n",
    "\n",
    "\n",
    "print(mqloss(y, y_pred_quantiles.permute(1,2,3,0).numpy(), quantiles))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
